---
title: "ML_Project"
output: html_document
---

Per prima cosa carichiamo le librerie che serviranno per lo svolgimento di questo progetto

```{r}

library(dplyr)
library(rpart)
library(caret)
library(e1071)
library(caTools)
library(rlist)
library(randomForest)
library(mlbench)

```
Adesso carichiamo il dataset 
Come vedrai dal codice, il primo elemento del comando "read.csv" mi indica il path per la collocazione del mio file "leaf.csv", adattalo secondo la tua collocazione del file.
Inoltre viene fornito a ogni colonna il nome. Ogni nome rappresenta un attributo della foglia, tali nomi vengono direttamente dal pdf "ReadMe" fornito con il download del dataset.

```{r}
leaf <- read.csv("C:\\Users\\apasc\\Desktop\\magistrale\\Introduction to machine learning\\project\\leaf.csv", header = FALSE,
                 col.names = c("Class", "Speciment_n°", "Eccentricity", "Aspect_Ratio",
                               "Elongation", "Solidity", "Stochastic_Convexity",
                               "Isoperimetric_Factor", "Maximal_Indentation_Depth",
                               "Lobedness", "Average_Intensity", "Average_Contrast",
                               "Smoothness", " Third_Moment", "Uniformity",
                               "Entropy"))
```

A partire dal dataset appena caricato viene rimossa la seconda colonna (ricordati ne aveva 16 di colonne gli attributi di prima) poichè essendo il Specimen Number è irrilevante ai fini dell'apprendimento. Vengono inoltre creati due nuovi dataset Train (contenente il 75% del dataset originale) e Test (contenente il restante 25%).
Il comando set.seed serve per quando si va a splittare il dataset, poichè la suddivisione in Train e Test è casuale, (cioè il 75% e il 25% non saranno mai uguali se li ripeto due volte)impostare quel valore di seed (che può essere anche un altro valore scegli tu alla fine gli dai un valore numerico) mi permette di replicare la stessa suddivisione di Train e Test (quindi con gli stessi valori) ogni volta che eseguirò il codice.(Per farla semplice quando tu eseguirai questo codice sul tuo pc otterai la mia stessa suddivisione pur essendo casuale, perchè abbiamo scelto come valore di seed radomico quello specificato nella parentesi). Quando faremo RF e SVM useremo lo stesso dataset di Train e Test per avere un confronto valido. Altrimenti si fa un confronto di accuratezza ed errore su due sub-dataset diversi.
Inoltre alla fine salvi i due file di Train e Test come file csv nel tuo pc (fai "getwd()") nella console per vedere il path dove li ha salvati

```{r}
set.seed(6528)
leaf <- leaf[,-2]
leaf$Class <- as.factor(leaf$Class)
#setting the test and train datasets, the later used for 
#CV and tuning
sp <- sample.split(leaf$Class, SplitRatio = 0.8)
Train <- subset(leaf, sp == TRUE)
Test <- subset(leaf, sp == FALSE)
write.csv(Train, file = "Train.csv")
write.csv(Test, file = "Test.csv")


```


Ora leggi i file salvati.
Iniziamo a fare RF.


```{r}
Train_rf <- read.csv("Train.csv", header = TRUE)
Test_rf <- read.csv("Test.csv", header = TRUE)
Train_rf <- Train_rf[,-1]
Test_rf <- Test_rf[,-1]
Train_rf$Class <- Train_rf$Class %>% as.factor
Test_rf$Class <- Test_rf$Class %>% as.factor
shuffled <- Train_rf[sample(nrow(Train_rf)),]


```

 Custom RF

```{r}
 
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("ntree", "mtry"), class = rep("numeric", 2),
                                  label = c("ntree", "mtry"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, ntree=param$ntree, mtry=param$mtry, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes


```

 Train model

```{r}
 
K <- 5
accuracy=rep(0,K)
mtry=rep(0,K)
ntree=rep(0,K)
n <- nrow(shuffled)
for(i in 1:K) {
  indexes <- ((i-1)*round(1/K*n) + 1):(i*round(1/K*n))
  if(exists("train_custom") && exists("test_custom")){
    rm(train_custom)
    rm(test_custom)
  }
  train_custom <- shuffled[-indexes,]
  test_custom <- shuffled[indexes,]
  print(nrow(train_custom) + nrow(test_custom) == nrow(shuffled))
  control <- trainControl(method="cv", number=3)
  tunegrid <- expand.grid(.ntree=c(250, 500, 750, 1000, 1250,1500), .mtry=c(2:6))
  custom <- train(Class~., data=train_custom, method=customRF, metric="Accuracy",
                  tuneGrid=tunegrid, trControl=control)
  mtry[i] <- custom$bestTune$mtry
  ntree[i] <- custom$bestTune$ntree
  randomF <- randomForest(Class~., data = train_custom, mtry = mtry[i],
                          ntree = ntree[i])
  prediction <- predict(randomF, test_custom[,-1], type="class")
  confMat <- table(test_custom$Class, prediction)
  accuracy[i] <- sum(diag(confMat))/sum(confMat)
}
avg_acc <- mean(accuracy)
sd_acc <- sd(accuracy)
write(paste("CV mean:", avg_acc, "; CV sd:", sd_acc), append = FALSE, file = "RF_result.txt")
write(c("Accuracies ", accuracy), append = TRUE, file = "RF_result.txt")
write(c("Best mtrys ", mtry), append = TRUE, file = "RF_result.txt")
write(c("Best ntrees ", ntree), append = TRUE, file = "RF_result.txt")

vect_of_matrix <- vector(mode = "list", length = K)
accuracies <- rep(0, K)

for(i in 1:K) {
  rf <- randomForest(Class~., data = Train_rf, mtry = mtry[i],
                     ntree = ntree[i])
  pr <- predict(rf, Test_rf[,-1], type="class")
  vect_of_matrix[[i]]<- table(pr, Test_rf$Class)
  accuracies[i] <- sum(diag(vect_of_matrix[[i]]))/sum(vect_of_matrix[[i]])
  write(paste("Accuracy with mtry = ", mtry[i], " ntree = ", ntree[i], ": ",
              accuracies[i]), append = TRUE, file = "RF_result.txt")
  
}

avg_acc_test <- accuracies %>% mean
sd_acc_test <- accuracies %>%  sd
classes <- leaf$Class %>% unique %>% sort %>% as.vector
best_acc <- accuracies %>% max
index <- match(best_acc, accuracies)
conf_matrix_best <- vect_of_matrix[[index]]
best_ntree <- ntree[index]
best_mtry <- mtry[index]


```   

Qua sotto verranno salvati dei file contenenti i risultati di RF sempre nella stessa cartella di prima (fai "getwd()" per capire il path per capire dove andare a cercarli).


```{r}
write(paste("Final test\n\tAvg:", avg_acc_test, " sd:", sd_acc_test), append = TRUE, file = "RF_result.txt")
write(paste("Best Accuracy:", best_acc, "with parameters ntree =", best_ntree, "and mtry =", best_mtry),
      append = TRUE, file = "RF_result.txt")

for (j in 1:30) {
  fp <- conf_matrix_best[j,-j] %>% sum
  tn <- conf_matrix_best[-j,-j] %>% diag %>% sum
  fpr <- fp/(fp+tn)
  
  fn <- conf_matrix_best[-j, j] %>% sum
  tp <- conf_matrix_best[j,j] 
  
  fnr <- fn/(fn+tp)
  
  write(paste("Class", classes[j], "\n\tFP=", fp, "TN=", tn, "FPR=", fpr, "\n\tFN=", fn, "TP=", tp, "FNR=", fnr),
        append = TRUE, file = "RF_result.txt")

}
```
Adesso praticamente il procedimento è lo stesso 
Viene impostato l'algoritmo SVM prima con kernel radiale poi polinomiale e poi si usa Nested per calcolare
i parametri di accuratezza e FPR ed FNR
Per Decision Tree invece verrà solo calcolata l'accuratezza.

Svm radiale

```{r}
#custom SVM
customSVM <- list(type = "Classification", library = "e1071", loop = NULL)
customSVM$parameters <- data.frame(parameter = c("cost", "gamma"), class = rep("numeric", 2),
                                   label = c("cost", "gamma"))
customSVM$grid <- function(x, y, len = NULL, search = "grid") {}
customSVM$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  svm(x, y, gamma=param$gamma, cost=param$cost, ...)
}
customSVM$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
customSVM$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
customSVM$sort <- function(x) x[order(x[,1]),]
customSVM$levels <- function(x) x$classes

set.seed(6528)
Train_svm <- read.csv("Train.csv", header = TRUE)
Test_svm <- read.csv("Test.csv", header = TRUE)
Train_svm <- Train_svm[,-1]
Test_svm <- Test_svm[,-1]
Train_svm$Class <- Train_svm$Class %>% as.factor
Test_svm$Class <- Test_svm$Class %>% as.factor
shuffled <- Train_svm[sample(nrow(Train_svm)),]

K <- 5
accuracy=rep(0,K)
gammas=rep(0,K)
costs=rep(0,K)
n <- nrow(shuffled)
dimX <- dim(shuffled[,-1])[2]

for(i in 1:K) {
  indexes <- ((i-1)*round(1/K*n) + 1):(i*round(1/K*n))
  if(exists("train_custom") && exists("test_custom")){
    rm(train_custom)
    rm(test_custom)
  }
  train_custom <- shuffled[-indexes,]
  test_custom <- shuffled[indexes,]
  print(nrow(train_custom) + nrow(test_custom) == nrow(shuffled))
  control <- trainControl(method="cv", number=3)
  tunegrid <- expand.grid(.gamma=c(2^(-4:5)/dimX),
                          .cost=c(2^(-8:8)))
  custom <- train(Class~., data=train_custom, method=customSVM, metric="Accuracy",
                  tuneGrid=tunegrid, trControl=control)
  gammas[i] <- custom$bestTune$gamma
  costs[i] <- custom$bestTune$cost
  svm <- svm(Class~., data = train_custom, gamma = gammas[i],
             cost = costs[i])
  prediction <- predict(svm, test_custom[,-1], type="class")
  confMat <- table(test_custom$Class, prediction)
  accuracy[i] <- sum(diag(confMat))/sum(confMat)
}
avg_acc <- mean(accuracy)
sd_acc <- sd(accuracy)
write(paste("CV mean:", avg_acc, "; CV sd:", sd_acc), append = FALSE, file = "SVM_result.txt")
write(c("Accuracies ", accuracy), append = TRUE, file = "SVM_result.txt")
write(c("Best gammas ", gammas), append = TRUE, file = "SVM_result.txt")
write(c("Best costs ", costs), append = TRUE, file = "SVM_result.txt")

vect_of_matrix <- vector(mode = "list", length = K)
accuracies <- rep(0, K)

for(i in 1:K) {
  svm <- svm(Class~., data = Train_svm, gamma = gammas[i],
             cost = costs[i])
  pr <- predict(svm, Test_svm[,-1], type="class")
  cat(pr)
  vect_of_matrix[[i]]<- table(pr, Test_svm$Class)
  accuracies[i] <- sum(diag(vect_of_matrix[[i]]))/sum(vect_of_matrix[[i]])
  write(paste("Accuracy with gamma = ", gammas[i], " cost = ", costs[i], ": ",
              accuracies[i]), append = TRUE, file = "SVM_result.txt")
  
}

avg_acc_test <- accuracies %>% mean
sd_acc_test <- accuracies %>%  sd
classes <- as.vector(leaf$Class %>% unique %>% sort)
best_acc <- accuracies %>% max
index <- match(best_acc, accuracies)
conf_matrix_best <- vect_of_matrix[[index]]
best_gamma <- gammas[index]
best_cost <- costs[index]

write(paste("Final test\n\tAvg:", avg_acc_test, " sd:", sd_acc_test), append = TRUE, file = "SVM_result.txt")
write(paste("Best Accuracy:", best_acc, "with parameters cost =", best_cost, "and gamma =", best_gamma),
      append = TRUE, file = "SVM_result.txt")

for (j in 1:30) {
  fp <- conf_matrix_best[j,-j] %>% sum
  tn <- conf_matrix_best[-j,-j] %>% diag %>% sum
  fpr <- fp/(fp+tn)
  
  fn <- conf_matrix_best[-j, j] %>% sum
  tp <- conf_matrix_best[j,j] 
  
  fnr <- fn/(fn+tp)
  
  write(paste("Class", classes[j], "\n\tFP=", fp, "TN=", tn, "FPR=", fpr, "\n\tFN=", fn, "TP=", tp, "FNR=", fnr),
        append = TRUE, file = "SVM_result.txt")
  
}

```
SVM polynomial

```{r}


#custom SVM
customSVM <- list(type = "Classification", library = "e1071", loop = NULL)
customSVM$parameters <- data.frame(parameter = c("deg", "gamma"), class = rep("numeric", 2),
                                   label = c("deg", "gamma"))
customSVM$grid <- function(x, y, len = NULL, search = "grid") {}
customSVM$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  svm(x, y, kernel="polynomial", gamma=param$gamma, degree=param$deg, ...)
}
customSVM$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
customSVM$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
customSVM$sort <- function(x) x[order(x[,1]),]
customSVM$levels <- function(x) x$classes

set.seed(6528)
Train_svm <- read.csv("Train.csv", header = TRUE)
Test_svm <- read.csv("Test.csv", header = TRUE)
Train_svm <- Train_svm[,-1]
Test_svm <- Test_svm[,-1]
Train_svm$Class <- Train_svm$Class %>% as.factor
Test_svm$Class <- Test_svm$Class %>% as.factor
shuffled <- Train_svm[sample(nrow(Train_svm)),]

K <- 5
accuracy=rep(0,K)
gammas=rep(0,K)
degrees=rep(0,K)
n <- nrow(shuffled)
dimX <- dim(shuffled[,-1])[2]
for(i in 1:K) {
  indexes <- ((i-1)*round(1/K*n) + 1):(i*round(1/K*n))
  if(exists("train_custom") && exists("test_custom")){
    rm(train_custom)
    rm(test_custom)
  }
  train_custom <- shuffled[-indexes,]
  test_custom <- shuffled[indexes,]
  print(nrow(train_custom) + nrow(test_custom) == nrow(shuffled))
  control <- trainControl(method="cv", number=3)
  tunegrid <- expand.grid(.gamma=c(0.1/dimX,(1/dimX)*(1:14),100),
                          .deg=c(1:8))
  custom <- train(Class~., data=train_custom, method=customSVM, metric="Accuracy",
                  tuneGrid=tunegrid, trControl=control)
  gammas[i] <- custom$bestTune$gamma
  degrees[i] <- custom$bestTune$deg
  svm <- svm(Class~., data = train_custom, gamma = gammas[i],
             degree = degrees[i], kernel = "polynomial")
  prediction <- predict(svm, test_custom[,-1], type="class")
  confMat <- table(test_custom$Class, prediction)
  accuracy[i] <- sum(diag(confMat))/sum(confMat)
}
avg_acc <- mean(accuracy)
sd_acc <- sd(accuracy)
write(paste("CV mean:", avg_acc, "; CV sd:", sd_acc), append = FALSE, file = "SVM_poly_result.txt")
write(c("Accuracies ", accuracy), append = TRUE, file = "SVM_poly_result.txt")
write(c("Best gammas ", gammas), append = TRUE, file = "SVM_poly_result.txt")
write(c("Best degrees ", degrees), append = TRUE, file = "SVM_poly_result.txt")

vect_of_matrix <- vector(mode = "list", length = K)
accuracies <- rep(0, K)

for(i in 1:K) {
  svm <- svm(Class~., data = Train_svm, gamma = gammas[i],
             degree = degrees[i], kernel = "polynomial")
  pr <- predict(svm, Test_svm[,-1], type="class")
  vect_of_matrix[[i]]<- table(pr, Test_svm$Class)
  accuracies[i] <- sum(diag(vect_of_matrix[[i]]))/sum(vect_of_matrix[[i]])
  write(paste("Accuracy with gamma = ", gammas[i], " degree = ", degrees[i], ": ",
              accuracies[i]), append = TRUE, file = "SVM_poly_result.txt")
  
}

avg_acc_test <- accuracies %>% mean
sd_acc_test <- accuracies %>%  sd
classes <- as.vector(leaf$Class %>% unique %>% sort)
best_acc <- accuracies %>% max
index <- match(best_acc, accuracies)
conf_matrix_best <- vect_of_matrix[[index]]
best_gamma <- gammas[index]
best_degree <- degrees[index]

write(paste("Final test\n\tAvg:", avg_acc_test, " sd:", sd_acc_test), append = TRUE, file = "SVM_poly_result.txt")
write(paste("Best Accuracy:", best_acc, "with parameters degree =", best_degree, "and gamma =", best_gamma),
      append = TRUE, file = "SVM_poly_result.txt")

for (j in 1:30) {
  fp <- conf_matrix_best[j,-j] %>% sum
  tn <- conf_matrix_best[-j,-j] %>% diag %>% sum
  fpr <- fp/(fp+tn)
  
  fn <- conf_matrix_best[-j, j] %>% sum
  tp <- conf_matrix_best[j,j] 
  
  fnr <- fn/(fn+tp)
  
  write(paste("Class", classes[j], "\n\tFP=", fp, "TN=", tn, "FPR=", fpr, "\n\tFN=", fn, "TP=", tp, "FNR=", fnr),
        append = TRUE, file = "SVM_poly_result.txt")
  
}



```



Decision Tree
```{r}


set.seed(6528)
Train_dt <- read.csv("Train.csv", header = TRUE)
Test_dt <- read.csv("Test.csv", header = TRUE)
Train_dt <- Train_dt[,-1]
Test_dt <- Test_dt[,-1]
Train_dt$Class <- Train_dt$Class %>% as.factor
Test_dt$Class <- Test_dt$Class %>% as.factor
shuffled <- Train_dt[sample(nrow(Train_dt)),]

K <- 5
accuracy <- rep(0,K)
cps <- rep(0,K)
for (i in 1:K) {
  # These indices indicate the interval of the test set
  indexes <- (((i-1) * round((1/K)*nrow(shuffled))) + 1):((i*round((1/K) * nrow(shuffled))))
  #take all the rows execpt those between 1:indices
  if(exists("train") && exists("test")){
    rm(train)
    rm(test)
  }
  train <- shuffled[-indexes,]
  #take all the rows with indices = 1:indices
  test <- shuffled[indexes,]
  print(nrow(train) + nrow(test) == nrow(shuffled))
  numFolds <- trainControl(method = "cv", number = 3)
  cpGrid <- expand.grid(.cp = seq(0.01, 0.2, 0.01))
  
  train_dt <- train(Class~., data = train, method = "rpart", metric="Accuracy",
                    trControl = numFolds, tuneGrid = cpGrid)
  cps[i] <- train_dt$bestTune$cp
  
  tree <- rpart(Class ~ ., train, method="class", cp = cps[i])
  pred <- predict(tree, test[,-1],type="class")
  confusionM <- table(test$Class, pred)
  accuracy[i] <- sum(diag(confusionM))/sum(confusionM)
}


write(paste("CV mean:", mean(accuracy), "; CV sd:", sd(accuracy)), append = FALSE, file = "DT_result.txt")
write(c("Accuracies ", accuracy), append = TRUE, file = "DT_result.txt")
write(c("Best cps ", cps), append = TRUE, file = "DT_result.txt")

accuracies <- rep(0, K)
for(i in 1:K) {
  rf <- rpart(Class~., data = Train_dt, method="class", cp = cps[i]) 
  pred_dt <- predict(rf, Test_dt[,-1], type="class")
  cf_dt <- table(Test_dt$Class, pred_dt)
  accuracies[i] <- sum(diag(cf_dt))/sum(cf_dt)
  write(paste("Acc with cp = ", cps[i], ": ", sum(diag(cf_dt))/sum(cf_dt)), append = TRUE, file = "DT_result.txt")
}
write(paste("Mean acc:", mean(accuracies), "sd:", sd(accuracies)))






```





